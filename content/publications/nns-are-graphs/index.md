---
title: "Neural Networks Are Graphs! Graph Neural Networks for Equivariant Processing of Neural Networks"

# Authors
# If you created a profile for a user (e.g. the default `admin` user), write the username (folder name) here
# and it will be replaced with their full name and linked to their profile.
authors:
- David W. Zhang
- admin
- Yan Zhang
- Yunlu Chen
- Gertjan J. Burghouts
- Cees G. M. Snoek

# Author notes (optional)
author_notes:
- University of Amsterdam
- University of Amsterdam
- Samsung - SAIT AI Lab
- University of Amsterdam
- TNO
- University of Amsterdam

date: "2023-07-01T00:00:00Z"
doi: ""

# Schedule page publish date (NOT publication's date).
publishDate: "2023-07-01T00:00:00Z"

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ["Workshop Paper"]

# Publication name and optional abbreviated publication name.
publication: In 2nd Annual Workshop on Topology, Algebra, and Geometry in Machine Learning (TAG-ML), ICML, 2023
publication_short: In TAG-ML, ICML, 2023*

abstract: Neural networks that can process the parameters of other neural networks find applications in diverse domains, including processing implicit neural representations, domain adaptation of pretrained networks, generating neural network weights, and predicting generalization errors. However, existing approaches either overlook the inherent permutation symmetry in the weight space or rely on intricate weight-sharing patterns to achieve equivariance. In this work, we propose representing neural networks as computation graphs, enabling the use of standard graph neural networks to preserve permutation symmetry. We also introduce probe features computed from the forward pass of the input neural network. Our proposed solution improves over prior methods from 86% to 97% accuracy on the challenging MNIST INR classification benchmark, showcasing the effectiveness of our approach.

# Summary. An optional shortened abstract.
summary: Neural Networks Are Graphs! Graph Neural Networks for Equivariant Processing of Neural Networks

tags:
  - Parameter-space Networks
  - Graph Neural Networks
  - Transformers
  - Equivariance
  - Neural Fields
  - Implicit Neural Representations

# Display this page in the Featured widget?
featured: true

# Custom links (uncomment lines below)
links:
- name: OpenReview
  url: https://openreview.net/forum?id=sCkLwG9wjy
- type: poster
  url: 'nns_are_graphs_poster.pdf'

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
image:
  caption: 'Latent Field Discovery in Interacting Dynamical Systems with Neural Fields'
  focal_point: ""
  preview_only: false
---
